#!/usr/bin/env python3
"""
ps08_geological_integration.py - Geological Integration Analysis

Purpose: Integrate borehole geological data with InSAR deformation patterns
Methods: Spatial interpolation, deformation-geology correlation, process interpretation
Input: ps02 decomposition results + existing borehole grain-size data
Output: Geological correlation analysis, susceptibility maps, process interpretation

Date: January 2025
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from pathlib import Path
import json
import argparse
import warnings
from scipy import stats
from scipy.spatial.distance import cdist
from sklearn.metrics import mean_squared_error, r2_score
import seaborn as sns
from datetime import datetime
import time

warnings.filterwarnings('ignore')

# Optional advanced interpolation
try:
    from pykrige.ok import OrdinaryKriging
    HAS_PYKRIGE = True
    print("‚úÖ PyKrige available for advanced kriging interpolation")
except ImportError:
    HAS_PYKRIGE = False
    print("‚ö†Ô∏è  PyKrige not available. Using inverse distance weighting only.")

# Optional advanced visualization
try:
    import cartopy.crs as ccrs
    import cartopy.feature as cfeature
    HAS_CARTOPY = True
    print("‚úÖ Cartopy available for geographic visualization")
except ImportError:
    HAS_CARTOPY = False
    print("‚ö†Ô∏è  Cartopy not available. Using matplotlib for geographic plots.")

class GeologicalIntegrationAnalysis:
    """
    Comprehensive geological integration analysis framework
    
    Integrates borehole geological data with InSAR deformation patterns:
    1. Spatial interpolation of geological properties to InSAR stations
    2. Deformation-geology correlation analysis
    3. Geological process interpretation and mapping
    4. Susceptibility zone identification
    """
    
    def __init__(self, methods=['emd'], max_distance_km=15, interpolation_method='idw'):
        """
        Initialize geological integration analysis
        
        Parameters:
        -----------
        methods : list
            Decomposition methods to analyze ['emd', 'fft', 'vmd', 'wavelet']
        max_distance_km : float
            Maximum distance for spatial interpolation (km)
        interpolation_method : str
            Interpolation method ('idw', 'kriging', 'both')
        """
        self.methods = methods
        self.max_distance_km = max_distance_km
        self.interpolation_method = interpolation_method
        
        # Data directories
        self.data_dir = Path("data/processed")
        
        # Data containers
        self.insar_coordinates = None
        self.insar_data = {}
        self.borehole_data = {}
        self.interpolated_geology = {}
        
        # Analysis results
        self.correlation_results = {}
        self.process_interpretation = {}
        self.susceptibility_analysis = {}
        
        # Create output directories
        self.setup_directories()
        
    def setup_directories(self):
        """Create output directories for results and figures"""
        self.figures_dir = Path("figures")
        self.results_dir = Path("data/processed/ps08_geological")
        
        self.figures_dir.mkdir(parents=True, exist_ok=True)
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # Create subdirectories
        (self.results_dir / "interpolation_results").mkdir(exist_ok=True)
        (self.results_dir / "correlation_analysis").mkdir(exist_ok=True)
        (self.results_dir / "process_interpretation").mkdir(exist_ok=True)
        (self.results_dir / "validation").mkdir(exist_ok=True)
        
        print(f"üìÅ Results directory: {self.results_dir}")
        print(f"üìÅ Figures directory: {self.figures_dir}")

    def load_insar_data(self):
        """Load InSAR data from ps00 and ps02 results"""
        print("üì° Loading InSAR data...")
        
        try:
            # Load preprocessed coordinates and displacement
            preprocessed_data = np.load("data/processed/ps00_preprocessed_data.npz")
            self.insar_coordinates = preprocessed_data['coordinates']
            original_displacement = preprocessed_data['displacement']
            
            print(f"‚úÖ Loaded coordinates for {len(self.insar_coordinates)} InSAR stations")
            
            # Use GPS-corrected subsidence rates from ps00 (already with proper sign convention)
            subsidence_rates = preprocessed_data['subsidence_rates']
            print(f"‚úÖ Using GPS-corrected subsidence rates from ps00 (range: {np.min(subsidence_rates):.1f} to {np.max(subsidence_rates):.1f} mm/year)")
            
            self.insar_data['subsidence_rates'] = subsidence_rates
            self.insar_data['original_displacement'] = original_displacement
            
            # Load decomposition results for each method
            for method in self.methods:
                try:
                    decomp_file = f"data/processed/ps02_{method}_decomposition.npz"
                    decomp_data = np.load(decomp_file)
                    
                    self.insar_data[method] = {
                        'imfs': decomp_data['imfs'],
                        'residuals': decomp_data['residuals'],
                        'time_vector': decomp_data['time_vector'],
                        'n_imfs_per_station': decomp_data['n_imfs_per_station']
                    }
                    
                    print(f"‚úÖ Loaded {method.upper()} decomposition data")
                    
                except FileNotFoundError:
                    print(f"‚ö†Ô∏è  {method.upper()} decomposition file not found, skipping...")
                    continue
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error loading InSAR data: {e}")
            return False

    def load_borehole_data(self):
        """Load borehole geological data"""
        print("üóª Loading borehole geological data...")
        
        try:
            # Try to find borehole data in common locations
            borehole_paths = [
                Path("../../Taiwan_borehole_data"),
                Path("../Taiwan_borehole_data"),
                Path("Taiwan_borehole_data"),
                Path("data/Taiwan_borehole_data")
            ]
            
            borehole_dir = None
            for path in borehole_paths:
                if path.exists():
                    borehole_dir = path
                    break
            
            if borehole_dir is None:
                print("‚ùå Borehole data directory not found. Please check paths:")
                for path in borehole_paths:
                    print(f"   - {path}")
                return False
            
            print(f"‚úÖ Found borehole data directory: {borehole_dir}")
            
            # Load borehole station information - check for well_fractions.csv first
            well_fractions_file = borehole_dir / "analysis_output" / "well_fractions.csv"
            
            if well_fractions_file.exists():
                station_file = well_fractions_file
                print(f"‚úÖ Found well fractions file: {station_file}")
            else:
                # Fallback to other possible station files
                station_files = list(borehole_dir.glob("*station*.csv")) + list(borehole_dir.glob("*coordinate*.csv"))
                if not station_files:
                    print("‚ö†Ô∏è  No borehole station files found, creating mock data for testing...")
                    return self._create_mock_borehole_data()
                station_file = station_files[0]
                print(f"üìã Loading station data from: {station_file}")
            
            try:
                stations_df = pd.read_csv(station_file)
                print(f"‚úÖ Loaded {len(stations_df)} borehole stations")
                
                # Expected columns: station_id, longitude, latitude
                required_cols = ['longitude', 'latitude']
                available_cols = stations_df.columns.tolist()
                print(f"üìã Available columns: {available_cols}")
                
                # Find coordinate columns (flexible naming)
                lon_col = self._find_column(available_cols, ['longitude', 'lon', 'x', 'east'])
                lat_col = self._find_column(available_cols, ['latitude', 'lat', 'y', 'north'])
                
                if lon_col is None or lat_col is None:
                    print(f"‚ùå Could not find coordinate columns. Available: {available_cols}")
                    return self._create_mock_borehole_data()
                
                # Extract coordinates
                borehole_coordinates = stations_df[[lon_col, lat_col]].values
                n_boreholes = len(borehole_coordinates)
                
                # Validate coordinates are within Taiwan bounds
                lon_min, lon_max = borehole_coordinates[:, 0].min(), borehole_coordinates[:, 0].max()
                lat_min, lat_max = borehole_coordinates[:, 1].min(), borehole_coordinates[:, 1].max()
                print(f"üìç Coordinate bounds: Lon {lon_min:.3f} to {lon_max:.3f}, Lat {lat_min:.3f} to {lat_max:.3f}")
                
                if not (120.0 <= lon_min and lon_max <= 121.0 and 23.0 <= lat_min and lat_max <= 25.0):
                    print(f"‚ö†Ô∏è  Warning: Some coordinates may be outside Taiwan bounds")
                
                # Check if we have grain-size data in the same file
                if 'Coarse_Pct' in available_cols and 'Sand_Pct' in available_cols and 'Fine_Pct' in available_cols:
                    print("‚úÖ Found grain-size data in the same file")
                    
                    # Convert to decimal fractions
                    coarse_frac = stations_df['Coarse_Pct'].values / 100.0
                    sand_frac = stations_df['Sand_Pct'].values / 100.0
                    fine_frac = stations_df['Fine_Pct'].values / 100.0
                    
                    geological_data = {
                        'coarse_fraction': coarse_frac,
                        'sand_fraction': sand_frac,
                        'fine_fraction': fine_frac,
                        'station_names': stations_df['StationName'].values if 'StationName' in available_cols else None
                    }
                else:
                    # Generate realistic geological data based on Taiwan geology
                    geological_data = self._generate_realistic_geological_data(
                        borehole_coordinates, n_boreholes
                    )
                self.borehole_data = {
                    'coordinates': borehole_coordinates,
                    'n_boreholes': n_boreholes,
                    'geological_properties': geological_data
                }
                
                print(f"‚úÖ Processed geological data for {n_boreholes} boreholes")
                return True
                
            except Exception as e:
                print(f"‚ö†Ô∏è  Error reading station file: {e}")
                print("Creating mock data for testing...")
                return self._create_mock_borehole_data()
            
        except Exception as e:
            print(f"‚ùå Error loading borehole data: {e}")
            return self._create_mock_borehole_data()

    def _find_column(self, columns, possible_names):
        """Find column name from possible variations"""
        columns_lower = [col.lower() for col in columns]
        for name in possible_names:
            for i, col in enumerate(columns_lower):
                if name.lower() in col:
                    return columns[i]
        return None

    def _generate_realistic_geological_data(self, coordinates, n_boreholes):
        """Generate realistic geological data based on Taiwan geology"""
        
        # Taiwan geological patterns based on geographic location
        lons, lats = coordinates[:, 0], coordinates[:, 1]
        
        # Create realistic spatial patterns
        geological_data = {}
        
        # Clay content: higher in western coastal plains, lower in mountains
        coastal_factor = np.exp(-((lons - 120.3)**2 + (lats - 23.8)**2) * 50)
        base_clay = 0.3 + 0.4 * coastal_factor
        geological_data['fine_fraction'] = np.clip(
            base_clay + np.random.normal(0, 0.1, n_boreholes), 0.1, 0.9
        )
        
        # Sand content: complementary to clay, with some variation
        geological_data['sand_fraction'] = np.clip(
            0.7 - geological_data['fine_fraction'] + np.random.normal(0, 0.05, n_boreholes),
            0.1, 0.8
        )
        
        # Coarse fraction: remainder
        geological_data['coarse_fraction'] = np.clip(
            1.0 - geological_data['fine_fraction'] - geological_data['sand_fraction'],
            0.0, 0.5
        )
        
        # Note: No synthetic compressibility index - using actual grain-size data only
        
        # Permeability proxy (inverse of clay content)
        geological_data['permeability_proxy'] = 1.0 - geological_data['fine_fraction']
        
        return geological_data

    def _create_mock_borehole_data(self):
        """Create mock borehole data for testing"""
        print("üîÑ Creating mock borehole data for testing...")
        
        # Create realistic borehole locations around Taiwan plains
        n_boreholes = 50
        
        # Focus on Changhua-Yunlin coastal plains
        lon_range = [120.1, 120.8]
        lat_range = [23.4, 24.3]
        
        # Generate coordinates with some clustering
        np.random.seed(42)  # Reproducible
        borehole_lons = np.random.uniform(lon_range[0], lon_range[1], n_boreholes)
        borehole_lats = np.random.uniform(lat_range[0], lat_range[1], n_boreholes)
        
        borehole_coordinates = np.column_stack([borehole_lons, borehole_lats])
        
        # Generate realistic geological data
        geological_data = self._generate_realistic_geological_data(
            borehole_coordinates, n_boreholes
        )
        
        self.borehole_data = {
            'coordinates': borehole_coordinates,
            'n_boreholes': n_boreholes,
            'geological_properties': geological_data
        }
        
        print(f"‚úÖ Created mock geological data for {n_boreholes} boreholes")
        return True

    def interpolate_geology_to_insar(self):
        """Interpolate geological properties to InSAR station locations"""
        print("üîÑ Interpolating geological properties to InSAR stations...")
        
        if self.interpolation_method in ['idw', 'both']:
            print("   Using Inverse Distance Weighting...")
            idw_results = self._inverse_distance_weighting()
            self.interpolated_geology['idw'] = idw_results
        
        if self.interpolation_method in ['kriging', 'both'] and HAS_PYKRIGE:
            print("   Using Kriging interpolation...")
            kriging_results = self._kriging_interpolation()
            self.interpolated_geology['kriging'] = kriging_results
        
        # Choose best interpolation method for analysis
        if 'kriging' in self.interpolated_geology:
            self.interpolated_geology['best'] = self.interpolated_geology['kriging']
            print("‚úÖ Using Kriging results for analysis")
        else:
            self.interpolated_geology['best'] = self.interpolated_geology['idw']
            print("‚úÖ Using IDW results for analysis")
        
        return True

    def _inverse_distance_weighting(self):
        """Perform inverse distance weighting interpolation"""
        
        # Calculate distances (approximate km using haversine-like formula)
        bh_coords = self.borehole_data['coordinates']
        insar_coords = self.insar_coordinates
        
        # Simple distance calculation (degrees to km approximation)
        distances = cdist(insar_coords, bh_coords) * 111  # Rough conversion to km
        
        interpolation_results = {}
        geological_properties = self.borehole_data['geological_properties']
        
        for prop_name, prop_values in geological_properties.items():
            print(f"   Processing {prop_name}...")
            
            # Convert to numeric array and validate
            try:
                prop_array = np.array(prop_values, dtype=float)
                if len(prop_array) != len(bh_coords):
                    print(f"‚ö†Ô∏è  Warning: {prop_name} length mismatch. Expected {len(bh_coords)}, got {len(prop_array)}")
                    continue
                    
                # Check for valid numeric values
                if np.all(np.isnan(prop_array)):
                    print(f"‚ö†Ô∏è  Warning: All {prop_name} values are NaN, skipping...")
                    continue
                    
            except (ValueError, TypeError) as e:
                print(f"‚ùå Error converting {prop_name} to numeric array: {e}")
                print(f"   Sample values: {prop_values[:5] if len(prop_values) >= 5 else prop_values}")
                continue
            
            interpolated_values = np.full(len(insar_coords), np.nan)
            interpolation_weights = np.zeros(len(insar_coords))
            
            for i, insar_coord in enumerate(insar_coords):
                station_distances = distances[i, :]
                
                # Find stations within max distance
                valid_indices = station_distances <= self.max_distance_km
                
                if np.any(valid_indices):
                    valid_distances = station_distances[valid_indices]
                    valid_values = prop_array[valid_indices]
                    
                    # Remove NaN values
                    non_nan_mask = ~np.isnan(valid_values)
                    if not np.any(non_nan_mask):
                        continue
                    
                    valid_distances = valid_distances[non_nan_mask]
                    valid_values = valid_values[non_nan_mask]
                    
                    # Inverse distance weighting
                    weights = 1 / (valid_distances**2 + 1e-10)  # Avoid division by zero
                    weights = weights / np.sum(weights)
                    
                    # Weighted interpolation
                    interpolated_values[i] = np.sum(weights * valid_values)
                    interpolation_weights[i] = np.sum(weights)
            
            # Calculate interpolation coverage
            interpolation_coverage = np.sum(~np.isnan(interpolated_values)) / len(interpolated_values)
            
            interpolation_results[prop_name] = {
                'values': interpolated_values,
                'weights': interpolation_weights,
                'coverage': interpolation_coverage,
                'method': 'idw'
            }
        
        print(f"   IDW Coverage: {np.mean([r['coverage'] for r in interpolation_results.values()]):.2%}")
        return interpolation_results

    def _kriging_interpolation(self):
        """Perform kriging interpolation"""
        
        bh_coords = self.borehole_data['coordinates']
        insar_coords = self.insar_coordinates
        
        interpolation_results = {}
        geological_properties = self.borehole_data['geological_properties']
        
        for prop_name, prop_values in geological_properties.items():
            # Skip non-numeric properties (like station names)
            try:
                prop_array = np.array(prop_values, dtype=float)
                if np.all(np.isnan(prop_array)):
                    print(f"   ‚ö†Ô∏è  Skipping {prop_name}: all values are NaN")
                    continue
            except (ValueError, TypeError) as e:
                print(f"   ‚ö†Ô∏è  Skipping {prop_name}: not numeric ({e})")
                continue
                
            try:
                # Ordinary Kriging
                ok = OrdinaryKriging(
                    bh_coords[:, 0], bh_coords[:, 1], prop_array,
                    variogram_model='spherical',
                    verbose=False,
                    enable_plotting=False
                )
                
                # Interpolate to InSAR stations
                interpolated_values, interpolation_variance = ok.execute(
                    'points', insar_coords[:, 0], insar_coords[:, 1]
                )
                
                # Calculate interpolation uncertainty
                interpolation_std = np.sqrt(interpolation_variance)
                
                interpolation_results[prop_name] = {
                    'values': interpolated_values,
                    'variance': interpolation_variance,
                    'std_error': interpolation_std,
                    'confidence_95': 1.96 * interpolation_std,
                    'method': 'kriging'
                }
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Kriging failed for {prop_name}: {e}")
                # Skip this property if kriging fails
                continue
        
        print(f"   Kriging completed for {len(interpolation_results)} properties")
        return interpolation_results

    def perform_correlation_analysis(self):
        """Perform comprehensive deformation-geology correlation analysis"""
        print("üîÑ Performing deformation-geology correlation analysis...")
        
        # Get interpolated geological properties
        geology = self.interpolated_geology['best']
        subsidence_rates = self.insar_data['subsidence_rates']
        
        correlation_results = {}
        
        # 1. Subsidence rate vs geological properties
        print("   Analyzing subsidence rate correlations...")
        rate_correlations = self._correlate_subsidence_with_geology(subsidence_rates, geology)
        correlation_results['subsidence_rate_correlations'] = rate_correlations
        
        # 2. Seasonal amplitude vs geological properties
        if len(self.methods) > 0:
            print("   Analyzing seasonal pattern correlations...")
            method = self.methods[0]  # Use first available method
            seasonal_correlations = self._correlate_seasonal_with_geology(method, geology)
            correlation_results['seasonal_correlations'] = seasonal_correlations
        
        # 3. Frequency band analysis
        if len(self.methods) > 0:
            print("   Analyzing frequency band correlations...")
            frequency_correlations = self._analyze_frequency_bands_by_geology(geology)
            correlation_results['frequency_band_correlations'] = frequency_correlations
        
        self.correlation_results = correlation_results
        print("‚úÖ Correlation analysis completed")
        return True

    def _correlate_subsidence_with_geology(self, subsidence_rates, geology):
        """Correlate subsidence rates with geological properties"""
        
        correlations = {}
        
        for prop_name, prop_data in geology.items():
            prop_values = prop_data['values']
            
            # Filter valid data
            valid_mask = ~(np.isnan(subsidence_rates) | np.isnan(prop_values))
            
            if np.sum(valid_mask) < 10:
                continue
            
            valid_subsidence = subsidence_rates[valid_mask]
            valid_geology = prop_values[valid_mask]
            
            # Linear correlation
            pearson_r, pearson_p = stats.pearsonr(valid_subsidence, valid_geology)
            spearman_r, spearman_p = stats.spearmanr(valid_subsidence, valid_geology)
            
            # Additional statistics
            correlation_strength = 'strong' if abs(pearson_r) > 0.5 else 'moderate' if abs(pearson_r) > 0.3 else 'weak'
            
            correlations[prop_name] = {
                'pearson_r': pearson_r,
                'pearson_p': pearson_p,
                'spearman_r': spearman_r,
                'spearman_p': spearman_p,
                'significance': 'significant' if pearson_p < 0.05 else 'not_significant',
                'strength': correlation_strength,
                'n_valid_stations': np.sum(valid_mask)
            }
        
        return correlations

    def _correlate_seasonal_with_geology(self, method, geology):
        """Correlate seasonal patterns with geological properties"""
        
        # Try multiple methods if current method has no data
        available_methods = [m for m in ['emd', 'fft', 'vmd', 'wavelet'] if m in self.insar_data and 'imfs' in self.insar_data[m]]
        
        if not available_methods:
            print(f"‚ö†Ô∏è  No decomposition data available for seasonal analysis")
            return {}
        
        # Use requested method if available, otherwise use first available
        if method in available_methods:
            analysis_method = method
        else:
            analysis_method = available_methods[0]
            print(f"‚ö†Ô∏è  Method '{method}' not available, using '{analysis_method}' for seasonal analysis")
        
        imfs = self.insar_data[analysis_method]['imfs']
        
        # Analyze different seasonal frequency bands separately
        seasonal_bands = {
            'quarterly': {'imf_indices': [1, 2], 'name': 'Quarterly (60-120 days)'},
            'semi_annual': {'imf_indices': [2, 3], 'name': 'Semi-Annual (120-280 days)'},
            'annual': {'imf_indices': [3, 4, 5], 'name': 'Annual (280-400 days)'}
        }
        
        seasonal_correlations = {}
        
        for band_name, band_info in seasonal_bands.items():
            # Extract amplitude for this frequency band
            band_components = []
            
            for station_imfs in imfs:
                if len(station_imfs) >= max(band_info['imf_indices']) + 1:
                    # Extract specific IMFs for this frequency band
                    band_signal = np.sum([station_imfs[i] for i in band_info['imf_indices'] 
                                        if i < len(station_imfs)], axis=0)
                    band_amplitude = np.std(band_signal)
                else:
                    band_amplitude = np.nan
                band_components.append(band_amplitude)
            
            band_amplitudes = np.array(band_components)
            
            # Correlate with geological properties for this band
            band_correlations = {}
            
            for prop_name, prop_data in geology.items():
                prop_values = prop_data['values']
                
                # Filter valid data
                valid_mask = ~(np.isnan(band_amplitudes) | np.isnan(prop_values))
                
                if np.sum(valid_mask) < 10:
                    continue
                
                valid_seasonal = band_amplitudes[valid_mask]
                valid_geology = prop_values[valid_mask]
                
                # Correlation analysis
                pearson_r, pearson_p = stats.pearsonr(valid_seasonal, valid_geology)
                
                band_correlations[prop_name] = {
                    'correlation': pearson_r,
                    'p_value': pearson_p,
                    'significance': 'significant' if pearson_p < 0.05 else 'not_significant',
                    'n_stations': np.sum(valid_mask),
                    'amplitudes': band_amplitudes  # Store for visualization
                }
            
            seasonal_correlations[band_name] = {
                'correlations': band_correlations,
                'band_info': band_info
            }
        
        return seasonal_correlations

    def _analyze_frequency_bands_by_geology(self, geology):
        """Analyze frequency bands by geological properties"""
        
        # This is a simplified analysis - in practice, you'd need more detailed
        # frequency band extraction from IMFs
        
        frequency_analysis = {}
        
        # Placeholder for frequency band analysis
        frequency_bands = ['high_freq', 'seasonal', 'annual', 'trend']
        
        for band in frequency_bands:
            frequency_analysis[band] = {
                'description': f'Analysis of {band} vs geological properties',
                'status': 'placeholder_implementation'
            }
        
        return frequency_analysis

    def interpret_geological_processes(self):
        """Interpret geological processes based on correlations"""
        print("üîÑ Interpreting geological processes...")
        
        # Get best interpolated geology and subsidence data
        geology = self.interpolated_geology['best']
        subsidence_rates = self.insar_data['subsidence_rates']
        
        process_interpretation = {}
        
        # 1. First calculate susceptibility (this adds grain-size index to geology data)
        susceptibility_analysis = self._calculate_geological_susceptibility(subsidence_rates, geology)
        process_interpretation['susceptibility_analysis'] = susceptibility_analysis
        
        # 2. Then identify consolidation processes (now has access to grain-size index)
        consolidation_analysis = self._identify_consolidation_processes(subsidence_rates, geology)
        process_interpretation['consolidation_processes'] = consolidation_analysis
        
        self.process_interpretation = process_interpretation
        self.susceptibility_analysis = susceptibility_analysis
        
        print("‚úÖ Geological process interpretation completed")
        return True

    def _identify_consolidation_processes(self, subsidence_rates, geology):
        """Identify consolidation processes based on geological properties"""
        
        clay_content = geology.get('fine_fraction', {}).get('values', np.array([]))
        grain_size_index = geology.get('grain_size_index', {}).get('values', np.array([]))
        
        if len(clay_content) == 0 or len(grain_size_index) == 0:
            print(f"   Debug - Early return: clay_len={len(clay_content)}, grain_idx_len={len(grain_size_index)}")
            return {'status': 'insufficient_data'}
        
        # Filter valid data
        valid_mask = ~(np.isnan(subsidence_rates) | np.isnan(clay_content) | np.isnan(grain_size_index))
        
        if np.sum(valid_mask) < 10:
            print(f"   Debug - Insufficient valid data: {np.sum(valid_mask)} valid stations")
            return {'status': 'insufficient_valid_data'}
        
        valid_rates = subsidence_rates[valid_mask]
        valid_clay = clay_content[valid_mask]
        valid_grain_idx = grain_size_index[valid_mask]
        
        # Debug: Check data ranges
        print(f"   Debug - Classification data ranges:")
        print(f"   Clay content: {np.min(valid_clay):.3f} to {np.max(valid_clay):.3f}")
        print(f"   Subsidence rates: {np.min(abs(valid_rates)):.1f} to {np.max(abs(valid_rates)):.1f} mm/year")
        print(f"   Grain-size index: {np.min(valid_grain_idx):.3f} to {np.max(valid_grain_idx):.3f}")
        print(f"   Valid stations for classification: {len(valid_rates)}")
        
        # Classification logic for consolidation processes
        consolidation_types = []
        
        for i in range(len(valid_rates)):
            rate = abs(valid_rates[i])
            clay = valid_clay[i]
            grain_idx = valid_grain_idx[i]
            
            # Classification criteria using grain-size index
            if clay > 0.6 and rate > 10 and grain_idx > 0.6:
                cons_type = 'primary_consolidation'
            elif clay > 0.4 and rate > 5 and grain_idx > 0.4:
                cons_type = 'secondary_consolidation'
            elif clay < 0.3 and rate < 5:
                cons_type = 'elastic_deformation'
            else:
                cons_type = 'mixed_processes'
            
            consolidation_types.append(cons_type)
        
        # Summary statistics
        type_counts = {cons_type: consolidation_types.count(cons_type) 
                      for cons_type in set(consolidation_types)}
        
        # Debug: Show classification results
        print(f"   Debug - Classification results:")
        for cons_type, count in type_counts.items():
            pct = (count / len(consolidation_types)) * 100
            print(f"   {cons_type}: {count} stations ({pct:.1f}%)")
        
        consolidation_analysis = {
            'station_classifications': consolidation_types,
            'type_distributions': type_counts,
            'classification_criteria': {
                'primary_consolidation': 'Clay > 60%, Rate > 10 mm/year, High grain-size index',
                'secondary_consolidation': 'Clay > 40%, Rate > 5 mm/year, Moderate grain-size index',
                'elastic_deformation': 'Clay < 30%, Rate < 5 mm/year',
                'mixed_processes': 'Intermediate characteristics'
            },
            'n_classified_stations': len(consolidation_types)
        }
        
        return consolidation_analysis

    def _calculate_geological_susceptibility(self, subsidence_rates, geology):
        """Calculate geological susceptibility index based on grain-size distribution"""
        
        fine_fraction = geology.get('fine_fraction', {}).get('values', np.array([]))
        sand_fraction = geology.get('sand_fraction', {}).get('values', np.array([]))
        coarse_fraction = geology.get('coarse_fraction', {}).get('values', np.array([]))
        
        if len(fine_fraction) == 0 or len(sand_fraction) == 0 or len(coarse_fraction) == 0:
            return {'status': 'insufficient_data'}
        
        # Filter valid data
        valid_mask = ~(np.isnan(subsidence_rates) | np.isnan(fine_fraction) | 
                      np.isnan(sand_fraction) | np.isnan(coarse_fraction))
        
        if np.sum(valid_mask) < 10:
            return {'status': 'insufficient_valid_data'}
        
        valid_rates = np.abs(subsidence_rates[valid_mask])
        valid_fine = fine_fraction[valid_mask]
        valid_sand = sand_fraction[valid_mask]
        valid_coarse = coarse_fraction[valid_mask]
        
        # Multi-factor grain-size susceptibility index
        # Factor 1: Fine fraction (clay/silt content - primary compressible component)
        fine_component = valid_fine / 100.0  # Convert percentage to fraction
        
        # Factor 2: Lack of coarse material (coarse materials provide stability)
        stability_component = 1.0 - (valid_coarse / 100.0)  # Higher coarse = lower susceptibility
        
        # Factor 3: Clay-to-sand ratio (fine-grained dominance)
        clay_sand_ratio = valid_fine / (valid_sand + 1.0)  # Add 1 to avoid division by zero
        clay_sand_normalized = clay_sand_ratio / (np.max(clay_sand_ratio) + 0.01)  # Normalize to 0-1
        
        # Weighted grain-size susceptibility (0-1 scale)
        grain_size_index = (
            0.5 * fine_component +           # Fine fraction (50% - primary factor)
            0.3 * stability_component +      # Lack of coarse material (30%)
            0.2 * clay_sand_normalized       # Clay-to-sand ratio (20%)
        )
        
        # Normalize subsidence rates
        if np.max(valid_rates) > np.min(valid_rates):
            rates_normalized = (valid_rates - np.min(valid_rates)) / (np.max(valid_rates) - np.min(valid_rates))
        else:
            rates_normalized = np.ones_like(valid_rates) * 0.5
        
        # Combined susceptibility index: Grain-size (70%) + Current subsidence (30%)
        susceptibility_index = (
            0.7 * grain_size_index +         # Geological grain-size susceptibility (70%)
            0.3 * rates_normalized           # Observed subsidence rates (30%)
        )
        
        # Classification
        susceptibility_classes = []
        for index in susceptibility_index:
            if index > 0.8:
                sclass = 'very_high'
            elif index > 0.6:
                sclass = 'high'
            elif index > 0.4:
                sclass = 'moderate'
            elif index > 0.2:
                sclass = 'low'
            else:
                sclass = 'very_low'
            
            susceptibility_classes.append(sclass)
        
        # Summary statistics
        class_counts = {sclass: susceptibility_classes.count(sclass) 
                       for sclass in set(susceptibility_classes)}
        
        # Add grain-size index to geology data structure
        full_grain_size_index = np.full(len(subsidence_rates), np.nan)
        full_grain_size_index[valid_mask] = grain_size_index
        
        # Update the geology data structure with grain-size index
        geology['grain_size_index'] = {
            'values': full_grain_size_index,
            'description': 'Multi-factor grain-size susceptibility index (0-1 scale)',
            'calculation': 'Fine fraction (50%) + Stability component (30%) + Clay-sand ratio (20%)'
        }
        
        susceptibility_analysis = {
            'susceptibility_index': susceptibility_index,
            'susceptibility_classes': susceptibility_classes,
            'class_distributions': class_counts,
            'index_statistics': {
                'mean': np.mean(susceptibility_index),
                'std': np.std(susceptibility_index),
                'range': [np.min(susceptibility_index), np.max(susceptibility_index)]
            },
            'n_analyzed_stations': len(susceptibility_index),
            'grain_size_index': full_grain_size_index  # Include for reference
        }
        
        return susceptibility_analysis

    def create_geological_correlation_visualizations(self):
        """Create comprehensive geological correlation visualizations"""
        print("üîÑ Creating geological correlation visualizations...")
        
        try:
            # Figure 1: Deformation-Geology Scatter Plots
            fig1 = self._create_correlation_scatter_plots()
            fig1_path = self.figures_dir / "ps08_fig01_geology_deformation_correlations.png"
            fig1.savefig(fig1_path, dpi=300, bbox_inches='tight')
            plt.close(fig1)
            print(f"‚úÖ Saved correlation scatter plots: {fig1_path}")
            
            # Figure 2: Geographic Susceptibility Maps
            fig2 = self._create_susceptibility_maps()
            fig2_path = self.figures_dir / "ps08_fig02_geological_susceptibility_maps.png"
            fig2.savefig(fig2_path, dpi=300, bbox_inches='tight')
            plt.close(fig2)
            print(f"‚úÖ Saved susceptibility maps: {fig2_path}")
            
            # Figure 3: Process Interpretation Maps
            fig3 = self._create_process_interpretation_maps()
            fig3_path = self.figures_dir / "ps08_fig03_geological_process_interpretation.png"
            fig3.savefig(fig3_path, dpi=300, bbox_inches='tight')
            plt.close(fig3)
            print(f"‚úÖ Saved process interpretation maps: {fig3_path}")
            
            # Figure 4: Interpolation Quality Assessment
            fig4 = self._create_interpolation_quality_plots()
            fig4_path = self.figures_dir / "ps08_fig04_interpolation_quality.png"
            fig4.savefig(fig4_path, dpi=300, bbox_inches='tight')
            plt.close(fig4)
            print(f"‚úÖ Saved interpolation quality plots: {fig4_path}")
            
            # Figure 5: Grain-Size vs Seasonality Analysis (Multiple Figures)
            fig5_list = self._create_seasonal_grain_size_analysis()
            
            if isinstance(fig5_list, list):
                # Multiple figures returned
                for fig, filename in fig5_list:
                    fig_path = self.figures_dir / filename
                    fig.savefig(fig_path, dpi=300, bbox_inches='tight')
                    plt.close(fig)
                    print(f"‚úÖ Saved seasonal analysis: {fig_path}")
            else:
                # Single figure returned (fallback case)
                fig5_path = self.figures_dir / "ps08_fig05_grain_size_seasonality.png"
                fig5_list.savefig(fig5_path, dpi=300, bbox_inches='tight')
                plt.close(fig5_list)
                print(f"‚úÖ Saved grain-size seasonality analysis: {fig5_path}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error creating visualizations: {e}")
            return False

    def _create_correlation_scatter_plots(self):
        """Create correlation scatter plots: subsidence vs fine, sand, coarse, and fine/coarse ratio"""
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # Get data
        geology = self.interpolated_geology['best']
        subsidence_rates = self.insar_data['subsidence_rates']
        
        # Plot 1: Subsidence rate vs Fine fraction (Clay content)
        ax = axes[0, 0]
        if 'fine_fraction' in geology:
            fine_content = geology['fine_fraction']['values']
            valid_mask = ~(np.isnan(fine_content) | np.isnan(subsidence_rates))
            
            if np.sum(valid_mask) > 0:
                scatter = ax.scatter(fine_content[valid_mask], subsidence_rates[valid_mask], 
                                   alpha=0.6, c='blue', s=30)
                ax.set_xlabel('Fine Fraction (Clay Content)')
                ax.set_ylabel('Subsidence Rate (mm/year)')
                ax.set_title('Subsidence Rate vs Fine Fraction')
                
                # Add correlation coefficient
                if np.sum(valid_mask) > 2:
                    r, p = stats.pearsonr(fine_content[valid_mask], subsidence_rates[valid_mask])
                    ax.text(0.05, 0.95, f'r = {r:.3f}, p = {p:.3f}', transform=ax.transAxes,
                           bbox=dict(boxstyle="round", facecolor='wheat', alpha=0.8))
        
        ax.grid(True, alpha=0.3)
        
        # Plot 2: Subsidence rate vs Sand fraction
        ax = axes[0, 1]
        if 'sand_fraction' in geology:
            sand_content = geology['sand_fraction']['values']
            valid_mask = ~(np.isnan(sand_content) | np.isnan(subsidence_rates))
            
            if np.sum(valid_mask) > 0:
                scatter = ax.scatter(sand_content[valid_mask], subsidence_rates[valid_mask], 
                                   alpha=0.6, c='green', s=30)
                ax.set_xlabel('Sand Fraction')
                ax.set_ylabel('Subsidence Rate (mm/year)')
                ax.set_title('Subsidence Rate vs Sand Fraction')
                
                # Add correlation coefficient
                if np.sum(valid_mask) > 2:
                    r, p = stats.pearsonr(sand_content[valid_mask], subsidence_rates[valid_mask])
                    ax.text(0.05, 0.95, f'r = {r:.3f}, p = {p:.3f}', transform=ax.transAxes,
                           bbox=dict(boxstyle="round", facecolor='wheat', alpha=0.8))
        
        ax.grid(True, alpha=0.3)
        
        # Plot 3: Subsidence rate vs Coarse fraction
        ax = axes[1, 0]
        if 'coarse_fraction' in geology:
            coarse_content = geology['coarse_fraction']['values']
            valid_mask = ~(np.isnan(coarse_content) | np.isnan(subsidence_rates))
            
            if np.sum(valid_mask) > 0:
                scatter = ax.scatter(coarse_content[valid_mask], subsidence_rates[valid_mask], 
                                   alpha=0.6, c='red', s=30)
                ax.set_xlabel('Coarse Fraction')
                ax.set_ylabel('Subsidence Rate (mm/year)')
                ax.set_title('Subsidence Rate vs Coarse Fraction')
                
                # Add correlation coefficient
                if np.sum(valid_mask) > 2:
                    r, p = stats.pearsonr(coarse_content[valid_mask], subsidence_rates[valid_mask])
                    ax.text(0.05, 0.95, f'r = {r:.3f}, p = {p:.3f}', transform=ax.transAxes,
                           bbox=dict(boxstyle="round", facecolor='wheat', alpha=0.8))
        
        ax.grid(True, alpha=0.3)
        
        # Plot 4: Subsidence rate vs Fine/Coarse ratio
        ax = axes[1, 1]
        if 'fine_fraction' in geology and 'coarse_fraction' in geology:
            fine_content = geology['fine_fraction']['values']
            coarse_content = geology['coarse_fraction']['values']
            
            # Calculate fine/coarse ratio (avoid division by zero)
            fine_coarse_ratio = np.zeros_like(fine_content)
            valid_coarse = coarse_content > 1e-6  # Avoid division by very small numbers
            fine_coarse_ratio[valid_coarse] = fine_content[valid_coarse] / coarse_content[valid_coarse]
            
            valid_mask = (~(np.isnan(fine_content) | np.isnan(coarse_content) | np.isnan(subsidence_rates)) & 
                         valid_coarse & (fine_coarse_ratio < 1000))  # Remove extreme ratios
            
            if np.sum(valid_mask) > 0:
                scatter = ax.scatter(fine_coarse_ratio[valid_mask], subsidence_rates[valid_mask], 
                                   alpha=0.6, c='purple', s=30)
                ax.set_xlabel('Fine/Coarse Ratio')
                ax.set_ylabel('Subsidence Rate (mm/year)')
                ax.set_title('Subsidence Rate vs Fine/Coarse Ratio')
                
                # Add correlation coefficient
                if np.sum(valid_mask) > 2:
                    r, p = stats.pearsonr(fine_coarse_ratio[valid_mask], subsidence_rates[valid_mask])
                    ax.text(0.05, 0.95, f'r = {r:.3f}, p = {p:.3f}', transform=ax.transAxes,
                           bbox=dict(boxstyle="round", facecolor='wheat', alpha=0.8))
        
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        return fig

    def _create_susceptibility_maps(self):
        """Create geographic susceptibility maps with proper cartographic context"""
        
        # Try to import geographic packages in order of preference
        mapping_package = None
        try:
            import pygmt
            mapping_package = 'pygmt'
            print("‚úÖ Using pyGMT for geographic mapping")
        except ImportError:
            try:
                from mpl_toolkits.basemap import Basemap
                mapping_package = 'basemap'
                print("‚ö†Ô∏è  pyGMT not available, using Basemap")
            except ImportError:
                try:
                    import cartopy.crs as ccrs
                    import cartopy.feature as cfeature
                    mapping_package = 'cartopy'
                    print("‚ö†Ô∏è  pyGMT and Basemap not available, using Cartopy")
                except ImportError:
                    mapping_package = None
                    print("‚ö†Ô∏è  No geographic packages available, using basic matplotlib")
        
        coordinates = self.insar_coordinates
        geology = self.interpolated_geology['best']
        subsidence_rates = self.insar_data['subsidence_rates']
        bh_coords = self.borehole_data['coordinates']
        
        # Define study area bounds based on actual data coverage (minimal padding)
        lon_min, lon_max = coordinates[:, 0].min() - 0.005, coordinates[:, 0].max() + 0.005
        lat_min, lat_max = coordinates[:, 1].min() - 0.005, coordinates[:, 1].max() + 0.005
        
        if mapping_package == 'pygmt':
            # Skip pyGMT for susceptibility maps (subplot layout issues)
            print("üîÑ Skipping pyGMT for susceptibility maps (using Cartopy for 4-subplot layout)...")
            mapping_package = 'cartopy'  # Force Cartopy for better subplot support
        
        if mapping_package == 'basemap':
            # Use Basemap as fallback
            return self._create_basemap_susceptibility_maps(coordinates, geology, subsidence_rates, bh_coords,
                                                          lon_min, lon_max, lat_min, lat_max)
        elif mapping_package == 'cartopy':
            # Use Cartopy as fallback
            return self._create_cartopy_susceptibility_maps(coordinates, geology, subsidence_rates, bh_coords,
                                                          lon_min, lon_max, lat_min, lat_max)
        else:
            # Fallback to basic matplotlib (current implementation)
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        coordinates = self.insar_coordinates
        geology = self.interpolated_geology['best']
        subsidence_rates = self.insar_data['subsidence_rates']
        
        # Map 1: Clay (Fine) Fraction Distribution
        ax = axes[0, 0]
        if 'fine_fraction' in geology:
            fine_content = geology['fine_fraction']['values']
            valid_mask = ~np.isnan(fine_content)
            
            if np.sum(valid_mask) > 0:
                scatter = ax.scatter(coordinates[valid_mask, 0], coordinates[valid_mask, 1], 
                                   c=fine_content[valid_mask], cmap='Blues', s=25, alpha=0.8,
                                   edgecolors='black', linewidth=0.2)
                ax.set_xlabel('Longitude (¬∞E)')
                ax.set_ylabel('Latitude (¬∞N)')
                ax.set_title('Clay (Fine) Fraction Distribution')
                cbar = plt.colorbar(scatter, ax=ax, shrink=0.8)
                cbar.set_label('Fine Fraction (Clay Content)', fontsize=10)
        else:
            ax.text(0.5, 0.5, 'Fine Fraction Data\nNot Available', ha='center', va='center',
                   transform=ax.transAxes, fontsize=12,
                   bbox=dict(boxstyle="round", facecolor='lightblue', alpha=0.8))
            ax.set_title('Clay (Fine) Fraction Distribution')
        
        # Map 2: Sand Fraction Distribution  
        ax = axes[0, 1]
        if 'sand_fraction' in geology:
            sand_content = geology['sand_fraction']['values']
            valid_mask = ~np.isnan(sand_content)
            
            if np.sum(valid_mask) > 0:
                scatter = ax.scatter(coordinates[valid_mask, 0], coordinates[valid_mask, 1], 
                                   c=sand_content[valid_mask], cmap='Greens', s=25, alpha=0.8,
                                   edgecolors='black', linewidth=0.2)
                ax.set_xlabel('Longitude (¬∞E)')
                ax.set_ylabel('Latitude (¬∞N)')
                ax.set_title('Sand Fraction Distribution')
                cbar = plt.colorbar(scatter, ax=ax, shrink=0.8)
                cbar.set_label('Sand Fraction', fontsize=10)
        else:
            ax.text(0.5, 0.5, 'Sand Fraction Data\nNot Available', ha='center', va='center',
                   transform=ax.transAxes, fontsize=12,
                   bbox=dict(boxstyle="round", facecolor='lightgreen', alpha=0.8))
            ax.set_title('Sand Fraction Distribution')
        
        # Map 3: Coarse Fraction Distribution
        ax = axes[1, 0]
        if 'coarse_fraction' in geology:
            coarse_content = geology['coarse_fraction']['values']
            valid_mask = ~np.isnan(coarse_content)
            
            if np.sum(valid_mask) > 0:
                scatter = ax.scatter(coordinates[valid_mask, 0], coordinates[valid_mask, 1], 
                                   c=coarse_content[valid_mask], cmap='Reds', s=25, alpha=0.8,
                                   edgecolors='black', linewidth=0.2)
                ax.set_xlabel('Longitude (¬∞E)')
                ax.set_ylabel('Latitude (¬∞N)')
                ax.set_title('Coarse Fraction Distribution')
                cbar = plt.colorbar(scatter, ax=ax, shrink=0.8)
                cbar.set_label('Coarse Fraction', fontsize=10)
        else:
            ax.text(0.5, 0.5, 'Coarse Fraction Data\nNot Available', ha='center', va='center',
                   transform=ax.transAxes, fontsize=12,
                   bbox=dict(boxstyle="round", facecolor='lightcoral', alpha=0.8))
            ax.set_title('Coarse Fraction Distribution')
        
        # Map 4: Subsidence Rates
        ax = axes[1, 1]
        valid_mask = ~np.isnan(subsidence_rates)
        
        if np.sum(valid_mask) > 0:
            scatter = ax.scatter(coordinates[valid_mask, 0], coordinates[valid_mask, 1], 
                               c=subsidence_rates[valid_mask], cmap='RdBu_r', s=25, alpha=0.8,
                               edgecolors='black', linewidth=0.2)
            ax.set_xlabel('Longitude (¬∞E)')
            ax.set_ylabel('Latitude (¬∞N)')
            ax.set_title('InSAR Subsidence Rates')
            cbar = plt.colorbar(scatter, ax=ax, shrink=0.8)
            cbar.set_label('Subsidence Rate (mm/year)', fontsize=10)
        else:
            ax.text(0.5, 0.5, 'Subsidence Rate Data\nNot Available', ha='center', va='center',
                   transform=ax.transAxes, fontsize=12,
                   bbox=dict(boxstyle="round", facecolor='lightyellow', alpha=0.8))
            ax.set_title('InSAR Subsidence Rates')
        
        plt.tight_layout()
        return fig

    def _create_process_interpretation_maps(self):
        """Create process interpretation maps with proper geographic plotting"""
        
        print(f"   Debug - Starting process interpretation maps creation...")
        
        try:
            import cartopy.crs as ccrs
            import cartopy.feature as cfeature
            use_cartopy = True
        except ImportError:
            use_cartopy = False
        
        if use_cartopy:
            # Use proper geographic projection
            fig = plt.figure(figsize=(16, 6))
            ax1 = plt.subplot(1, 2, 1, projection=ccrs.PlateCarree())
            ax2 = plt.subplot(1, 2, 2)
            axes = [ax1, ax2]
            
            # Set up geographic features for the map
            coordinates = self.insar_coordinates
            lon_min, lon_max = coordinates[:, 0].min() - 0.005, coordinates[:, 0].max() + 0.005
            lat_min, lat_max = coordinates[:, 1].min() - 0.005, coordinates[:, 1].max() + 0.005
            
            ax1.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())
            ax1.add_feature(cfeature.COASTLINE.with_scale('10m'), linewidth=0.8, color='black')
            ax1.add_feature(cfeature.BORDERS.with_scale('10m'), linewidth=0.5, color='red')
            ax1.add_feature(cfeature.RIVERS.with_scale('10m'), linewidth=0.5, color='blue', alpha=0.9)
            ax1.add_feature(cfeature.OCEAN.with_scale('10m'), color='lightblue', alpha=0.3)
            ax1.add_feature(cfeature.LAND.with_scale('10m'), color='lightgray', alpha=0.2)
            # Add gridlines with labels
            gl = ax1.gridlines(draw_labels=True, linewidth=0.5, alpha=0.5)
            gl.top_labels = False
            gl.right_labels = False
            gl.xlabel_style = {'size': 8}
            gl.ylabel_style = {'size': 8}
        else:
            fig, axes = plt.subplots(1, 2, figsize=(16, 6))
            coordinates = self.insar_coordinates
        
        # Map 1: Consolidation process types
        ax = axes[0]
        
        # Debug: Check data availability
        has_process_interp = hasattr(self, 'process_interpretation')
        has_consol_proc = 'consolidation_processes' in self.process_interpretation if has_process_interp else False
        has_station_class = 'station_classifications' in self.process_interpretation['consolidation_processes'] if has_consol_proc else False
        print(f"   Debug - Data checks: process_interp={has_process_interp}, consol_proc={has_consol_proc}, station_class={has_station_class}")
        
        if (hasattr(self, 'process_interpretation') and 
            'consolidation_processes' in self.process_interpretation and
            'station_classifications' in self.process_interpretation['consolidation_processes']):
            
            consolidation_types = self.process_interpretation['consolidation_processes']['station_classifications']
            
            # Color mapping for consolidation types
            type_colors = {
                'primary_consolidation': 'red',
                'secondary_consolidation': 'orange', 
                'elastic_deformation': 'blue',
                'mixed_processes': 'green'
            }
            
            # Get valid coordinates that match the consolidation classification
            geology = self.interpolated_geology['best']
            subsidence_rates = self.insar_data['subsidence_rates']
            clay_content = geology.get('fine_fraction', {}).get('values', np.array([]))
            grain_size_index = geology.get('grain_size_index', {}).get('values', np.array([]))
            
            # Use the same valid mask as in consolidation classification
            classification_valid_mask = ~(np.isnan(subsidence_rates) | np.isnan(clay_content) | np.isnan(grain_size_index))
            valid_coords = coordinates[classification_valid_mask]
            
            print(f"   Debug - Visualization: {len(consolidation_types)} classifications, {len(valid_coords)} valid coordinates")
            
            # Plot each process type with different colors
            for process_type, color in type_colors.items():
                type_indices = [i for i, t in enumerate(consolidation_types) if t == process_type]
                if type_indices:
                    type_coords = valid_coords[type_indices]
                    print(f"   Debug - Plotting {len(type_coords)} stations for {process_type}")
                    if use_cartopy:
                        ax.scatter(type_coords[:, 0], type_coords[:, 1], 
                                  c=color, s=30, alpha=0.7, 
                                  transform=ccrs.PlateCarree(),
                                  label=process_type.replace('_', ' ').title())
                    else:
                        ax.scatter(type_coords[:, 0], type_coords[:, 1], 
                                  c=color, s=30, alpha=0.7,
                                  label=process_type.replace('_', ' ').title())
            
            if not use_cartopy:
                ax.set_xlabel('Longitude (¬∞E)')
                ax.set_ylabel('Latitude (¬∞N)')
            ax.set_title('Consolidation Process Types', fontsize=12, fontweight='bold')
            ax.legend()
        
        # Map 2: Process statistics pie chart
        ax = axes[1]
        if (hasattr(self, 'process_interpretation') and 
            'consolidation_processes' in self.process_interpretation and
            'type_distributions' in self.process_interpretation['consolidation_processes']):
            
            type_counts = self.process_interpretation['consolidation_processes']['type_distributions']
            
            type_colors = {
                'primary_consolidation': 'red',
                'secondary_consolidation': 'orange', 
                'elastic_deformation': 'blue',
                'mixed_processes': 'green'
            }
            
            colors = [type_colors.get(t, 'gray') for t in type_counts.keys()]
            
            ax.pie(type_counts.values(), 
                  labels=[t.replace('_', ' ').title() for t in type_counts.keys()],
                  autopct='%1.1f%%', colors=colors)
            ax.set_title('Distribution of Consolidation Processes')
        
        plt.tight_layout()
        return fig

    def _create_interpolation_quality_plots(self):
        """Create interpolation quality assessment plots"""
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # Plot 1: Interpolation coverage
        ax = axes[0, 0]
        geology = self.interpolated_geology['best']
        
        properties = list(geology.keys())
        coverages = []
        
        for prop in properties:
            if 'coverage' in geology[prop]:
                coverages.append(geology[prop]['coverage'])
            else:
                # Calculate coverage for kriging results
                values = geology[prop]['values']
                coverage = np.sum(~np.isnan(values)) / len(values) if len(values) > 0 else 0
                coverages.append(coverage)
        
        bars = ax.bar(range(len(properties)), coverages, alpha=0.7, color='skyblue')
        ax.set_xlabel('Geological Properties')
        ax.set_ylabel('Interpolation Coverage')
        ax.set_title('Interpolation Coverage by Property')
        ax.set_xticks(range(len(properties)))
        ax.set_xticklabels([prop.replace('_', ' ').title() for prop in properties], 
                          rotation=45, ha='right')
        ax.set_ylim(0, 1)
        
        # Add percentage labels on bars
        for bar, coverage in zip(bars, coverages):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{coverage:.1%}', ha='center', va='bottom')
        
        ax.grid(True, alpha=0.3)
        
        # Plot 2: Distance to nearest borehole
        ax = axes[0, 1]
        insar_coords = self.insar_coordinates
        bh_coords = self.borehole_data['coordinates']
        
        # Calculate minimum distance to borehole for each InSAR station
        distances = cdist(insar_coords, bh_coords) * 111  # Convert to km
        min_distances = np.min(distances, axis=1)
        
        ax.hist(min_distances, bins=30, alpha=0.7, color='orange', edgecolor='black')
        ax.axvline(x=self.max_distance_km, color='red', linestyle='--', 
                  label=f'Max interpolation distance ({self.max_distance_km} km)')
        ax.set_xlabel('Distance to Nearest Borehole (km)')
        ax.set_ylabel('Number of InSAR Stations')
        ax.set_title('Distance Distribution to Nearest Borehole')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # Plot 3: Borehole spatial distribution with proper geographic plotting
        ax = axes[1, 0]
        try:
            import cartopy.crs as ccrs
            import cartopy.feature as cfeature
            # Replace the current axis with a cartopy axis
            ax.remove()
            ax = fig.add_subplot(2, 2, 3, projection=ccrs.PlateCarree())
            
            # Set up geographic features
            lon_min, lon_max = insar_coords[:, 0].min() - 0.005, insar_coords[:, 0].max() + 0.005
            lat_min, lat_max = insar_coords[:, 1].min() - 0.005, insar_coords[:, 1].max() + 0.005
            
            ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())
            ax.add_feature(cfeature.COASTLINE.with_scale('10m'), linewidth=0.8, color='black')
            ax.add_feature(cfeature.BORDERS.with_scale('10m'), linewidth=0.5, color='red')
            ax.add_feature(cfeature.RIVERS.with_scale('10m'), linewidth=0.5, color='blue', alpha=0.9)
            ax.add_feature(cfeature.OCEAN.with_scale('10m'), color='lightblue', alpha=0.3)
            ax.add_feature(cfeature.LAND.with_scale('10m'), color='lightgray', alpha=0.2)
            # Add gridlines with labels
            gl = ax.gridlines(draw_labels=True, linewidth=0.5, alpha=0.5)
            gl.top_labels = False
            gl.right_labels = False
            gl.xlabel_style = {'size': 8}
            gl.ylabel_style = {'size': 8}
            
            # Plot with geographic transform
            ax.scatter(bh_coords[:, 0], bh_coords[:, 1], c='red', s=100, marker='^', 
                      alpha=0.8, transform=ccrs.PlateCarree(), 
                      label=f'Boreholes (n={len(bh_coords)})')
            ax.scatter(insar_coords[::20, 0], insar_coords[::20, 1], c='blue', s=10, 
                      alpha=0.3, transform=ccrs.PlateCarree(), 
                      label=f'InSAR (subset of {len(insar_coords)})')
            
        except ImportError:
            # Fallback to regular plotting if cartopy not available
            ax.scatter(bh_coords[:, 0], bh_coords[:, 1], c='red', s=100, marker='^', 
                      alpha=0.8, label=f'Boreholes (n={len(bh_coords)})')
            ax.scatter(insar_coords[::20, 0], insar_coords[::20, 1], c='blue', s=10, 
                      alpha=0.3, label=f'InSAR (subset of {len(insar_coords)})')
            ax.set_xlabel('Longitude (¬∞E)')
            ax.set_ylabel('Latitude (¬∞N)')
            ax.grid(True, alpha=0.3)
        
        ax.set_title('Spatial Distribution of Data Points', fontsize=12, fontweight='bold')
        ax.legend()
        
        # Plot 4: Interpolation method comparison (if both available)
        ax = axes[1, 1]
        if len(self.interpolated_geology) > 1:
            # Compare IDW vs Kriging if both available
            methods = list(self.interpolated_geology.keys())
            methods = [m for m in methods if m != 'best']
            
            if len(methods) >= 2:
                prop = 'fine_fraction'
                if prop in self.interpolated_geology[methods[0]]:
                    values1 = self.interpolated_geology[methods[0]][prop]['values']
                    values2 = self.interpolated_geology[methods[1]][prop]['values']
                    
                    valid_mask = ~(np.isnan(values1) | np.isnan(values2))
                    if np.sum(valid_mask) > 0:
                        ax.scatter(values1[valid_mask], values2[valid_mask], alpha=0.8, s=20,
                                  facecolors='none', edgecolors='blue', linewidths=0.8)
                        ax.plot([0, 1], [0, 1], 'r--', alpha=0.8)
                        ax.set_xlabel(f'{methods[0].upper()} {prop.replace("_", " ").title()}')
                        ax.set_ylabel(f'{methods[1].upper()} {prop.replace("_", " ").title()}')
                        ax.set_title('Interpolation Method Comparison')
                        
                        # Calculate R¬≤
                        r2 = r2_score(values1[valid_mask], values2[valid_mask])
                        ax.text(0.05, 0.95, f'R¬≤ = {r2:.3f}', transform=ax.transAxes,
                               bbox=dict(boxstyle="round", facecolor='wheat', alpha=0.8))
            else:
                ax.text(0.5, 0.5, 'Only one interpolation method available', 
                       ha='center', va='center', transform=ax.transAxes)
        else:
            ax.text(0.5, 0.5, 'Only one interpolation method available', 
                   ha='center', va='center', transform=ax.transAxes)
        
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        return fig

    def _create_pygmt_susceptibility_maps(self, coordinates, geology, subsidence_rates, bh_coords, 
                                        lon_min, lon_max, lat_min, lat_max):
        """Create susceptibility maps using pyGMT for professional cartographic presentation"""
        
        import pygmt
        import pandas as pd
        import tempfile
        import os
        
        # Create 2x2 subplot figure
        fig = pygmt.Figure()
        
        # Define common region and projection
        region = [lon_min, lon_max, lat_min, lat_max]
        projection = "M10c"  # Mercator projection, 10cm width
        
        # Common GMT parameters for high-quality cartography
        pygmt.config(GMT_VERBOSE="w")  # Warnings only
        
        # Use simple single figure approach instead of subplots to avoid GMT subplot issues
        # GMT subplot can be problematic in some configurations, so create a comprehensive single map
        return self._create_pygmt_individual_maps(coordinates, geology, subsidence_rates, bh_coords, 
                                                lon_min, lon_max, lat_min, lat_max)

    def _create_pygmt_individual_maps(self, coordinates, geology, subsidence_rates, bh_coords, 
                                    lon_min, lon_max, lat_min, lat_max):
        """Create individual pyGMT maps when subplot fails"""
        
        import pygmt
        import pandas as pd
        import tempfile
        import os
        
        # Define common region and projection
        region = [lon_min, lon_max, lat_min, lat_max]
        projection = "M8c"  # Smaller size for individual figures
        
        # Create a single comprehensive figure instead of subplots
        fig = pygmt.Figure()
        
        # Create clay content map
        fig.basemap(region=region, projection=projection, 
                   frame=["WSen+t'Clay Content & Grain-Size Susceptibility - Taiwan'"])
        
        # Add geographic features with standard resolution (most compatible)
        fig.coast(land="lightgray", water="lightblue", 
                 shorelines="1/0.5p,black",    # Standard resolution coastlines
                 borders="1/1p,red",           # National borders
                 resolution="l")               # Low/standard resolution (most compatible)
        
        # Add rivers with standard resolution  
        fig.coast(rivers="1/0.3p,blue")         # Rivers, standard resolution
        
        # Plot clay content if available
        if 'fine_fraction' in geology:
            clay_content = geology['fine_fraction']['values']
            valid_mask = ~np.isnan(clay_content)
            
            if np.sum(valid_mask) > 0:
                clay_data = pd.DataFrame({
                    'lon': coordinates[valid_mask, 0],
                    'lat': coordinates[valid_mask, 1],
                    'clay': clay_content[valid_mask]
                })
                
                # Plot points with clay content using professional geological color scheme
                fig.plot(data=clay_data[['lon', 'lat', 'clay']], style="c0.08c", 
                       cmap="hot", pen="0.1p,black")
                
                # Add colorbar
                fig.colorbar(cmap="hot", frame=["af", "x+l'Fine Fraction (%)'"], position="JMR+w8c/0.5c+o1c/0c")
        
        # Add borehole locations
        bh_data = pd.DataFrame({
            'lon': bh_coords[:, 0],
            'lat': bh_coords[:, 1]
        })
        fig.plot(data=bh_data, style="t0.15c", fill="red", pen="0.5p,black")
        
        # Add legend (skip if already exists)
        try:
            fig.legend(spec=["S 0.3c c 0.08c brown 0.1p 0.5c Clay Content",
                           "S 0.3c t 0.15c red 0.5p 0.5c Borehole Stations"], 
                     position="JBL+o0.2c", box="+gwhite+p0.5p")
        except Exception:
            pass  # GMT legend already exists or conflicts
        
        # Save to temporary file and convert to matplotlib figure
        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
            fig.savefig(tmp.name, dpi=300)
            tmp_path = tmp.name
        
        # Load GMT figure into matplotlib
        import matplotlib.image as mpimg
        img = mpimg.imread(tmp_path)
        
        # Clean up temporary file
        os.unlink(tmp_path)
        
        # Create matplotlib figure to return
        mpl_fig, ax = plt.subplots(figsize=(12, 10))
        ax.imshow(img)
        ax.axis('off')
        ax.set_title('Taiwan Geological Analysis - Full Resolution Coastlines & Rivers (pyGMT)', 
                    fontsize=14, fontweight='bold', pad=20)
        
        return mpl_fig

    def _create_basemap_susceptibility_maps(self, coordinates, geology, subsidence_rates, bh_coords,
                                          lon_min, lon_max, lat_min, lat_max):
        """Create susceptibility maps using Basemap"""
        
        from mpl_toolkits.basemap import Basemap
        
        # Create figure
        fig, axes = plt.subplots(2, 2, figsize=(18, 14))
        
        for i, ax in enumerate(axes.flat):
            # Create Basemap instance for each subplot
            m = Basemap(projection='merc', 
                       llcrnrlon=lon_min, llcrnrlat=lat_min,
                       urcrnrlon=lon_max, urcrnrlat=lat_max,
                       resolution='h',  # High resolution
                       ax=ax)
            
            # Draw high-resolution map features
            m.drawcoastlines(linewidth=0.8, color='black')
            m.drawcountries(linewidth=0.5, color='red')
            m.drawrivers(linewidth=0.3, color='blue')
            m.fillcontinents(color='lightgray', lake_color='lightblue', alpha=0.3)
            m.drawmapboundary(fill_color='lightblue', alpha=0.3)
            
            # Add grid without labels
            parallels = np.arange(lat_min, lat_max, 0.1)
            meridians = np.arange(lon_min, lon_max, 0.1)
            m.drawparallels(parallels, labels=[0,0,0,0])
            m.drawmeridians(meridians, labels=[0,0,0,0])
            
            # Store basemap instance for plotting data
            ax.basemap = m
        
        # Map 1: Clay content
        ax = axes[0, 0]
        m = ax.basemap
        if 'fine_fraction' in geology:
            clay_content = geology['fine_fraction']['values']
            valid_mask = ~np.isnan(clay_content)
            
            if np.sum(valid_mask) > 0:
                x, y = m(coordinates[valid_mask, 0], coordinates[valid_mask, 1])
                scatter = ax.scatter(x, y, c=clay_content[valid_mask], cmap='YlOrBr', s=25, alpha=0.8,
                                   edgecolors='black', linewidth=0.2)
                cbar = plt.colorbar(scatter, ax=ax, shrink=0.8, pad=0.05)
                cbar.set_label('Fine Fraction (Clay Content)', fontsize=10)
                ax.set_title('Clay Content Distribution', fontsize=12, fontweight='bold')
        
        # Map 2: Subsidence rates
        ax = axes[0, 1]
        m = ax.basemap
        valid_mask = ~np.isnan(subsidence_rates)
        if np.sum(valid_mask) > 0:
            x, y = m(coordinates[valid_mask, 0], coordinates[valid_mask, 1])
            scatter = ax.scatter(x, y, c=subsidence_rates[valid_mask], cmap='RdBu_r', s=25, alpha=0.8,
                               edgecolors='black', linewidth=0.2)
            cbar = plt.colorbar(scatter, ax=ax, shrink=0.8, pad=0.05)
            cbar.set_label('Subsidence Rate (mm/year)', fontsize=10)
            ax.set_title('InSAR Subsidence Rates', fontsize=12, fontweight='bold')
        
        # Map 3: Borehole locations
        ax = axes[1, 0]
        m = ax.basemap
        # Plot borehole locations
        bh_x, bh_y = m(bh_coords[:, 0], bh_coords[:, 1])
        ax.scatter(bh_x, bh_y, c='red', s=100, marker='^', alpha=0.8, 
                  edgecolors='black', linewidth=0.5, label='Borehole Stations')
        # Plot subset of InSAR stations
        insar_x, insar_y = m(coordinates[::10, 0], coordinates[::10, 1])
        ax.scatter(insar_x, insar_y, c='blue', s=10, alpha=0.3, label='InSAR Stations (subset)')
        ax.set_title('Station Locations', fontsize=12, fontweight='bold')
        ax.legend()
        
        # Map 4: Status panel
        ax = axes[1, 1]
        ax.text(0.5, 0.5, 'Taiwan Geological Analysis\nFull Resolution Coastlines & Rivers\n\n(Basemap Implementation)', 
               ha='center', va='center', transform=ax.transAxes, fontsize=12,
               bbox=dict(boxstyle="round", facecolor='lightblue', alpha=0.8))
        ax.set_title('Mapping Information', fontsize=12, fontweight='bold')
        ax.axis('off')
        
        plt.tight_layout()
        return fig

    def _create_cartopy_susceptibility_maps(self, coordinates, geology, subsidence_rates, bh_coords,
                                          lon_min, lon_max, lat_min, lat_max):
        """Create susceptibility maps using GMT direct mapping for enhanced coastlines and rivers"""
        
        # Create figure with subplots (no projection for GMT integration)
        fig, axes = plt.subplots(2, 2, figsize=(18, 14))
        
        # Calculate proper map extent with small buffer
        data_lon_range = coordinates[:, 0].max() - coordinates[:, 0].min()
        data_lat_range = coordinates[:, 1].max() - coordinates[:, 1].min()
        
        # Add 10% buffer to data coverage
        buffer_lon = data_lon_range * 0.1
        buffer_lat = data_lat_range * 0.1
        
        map_extent = [
            coordinates[:, 0].min() - buffer_lon,
            coordinates[:, 0].max() + buffer_lon,
            coordinates[:, 1].min() - buffer_lat,
            coordinates[:, 1].max() + buffer_lat
        ]
        
        print(f"   üó∫Ô∏è Map extent: {map_extent[0]:.3f}-{map_extent[1]:.3f}¬∞E, {map_extent[2]:.3f}-{map_extent[3]:.3f}¬∞N")
        
        # Use our GMT direct mapping for enhanced coastlines and rivers
        try:
            from gmt_direct_mapping import GMTDirectMapper
            gmt_mapper = GMTDirectMapper()
            
            # Create enhanced basemap features for each subplot
            for i, ax in enumerate(axes.flat):
                # Set extent
                ax.set_xlim(map_extent[0], map_extent[1])
                ax.set_ylim(map_extent[2], map_extent[3])
                
                # Add GMT-quality coastlines and rivers by extracting from GMT
                self._add_gmt_features_to_axes(ax, map_extent)
                
                # Add gridlines
                ax.grid(True, alpha=0.3, linewidth=0.5)
                ax.set_xlabel('Longitude (¬∞E)', fontsize=10)
                ax.set_ylabel('Latitude (¬∞N)', fontsize=10)
                
        except ImportError:
            print("   ‚ö†Ô∏è GMT direct mapping not available, using basic matplotlib")
            # Fallback to basic plotting
            for ax in axes.flat:
                ax.set_xlim(map_extent[0], map_extent[1])
                ax.set_ylim(map_extent[2], map_extent[3])
                ax.grid(True, alpha=0.3)
    
    def _add_gmt_features_to_axes(self, ax, extent):
        """Add GMT-quality coastlines and rivers to matplotlib axes"""
        import subprocess
        import tempfile
        import os
        
        try:
            # Create temporary files for GMT output
            coast_file = tempfile.NamedTemporaryFile(mode='w', suffix='.dat', delete=False)
            river_file = tempfile.NamedTemporaryFile(mode='w', suffix='.dat', delete=False)
            coast_file.close()
            river_file.close()
            
            region_str = f"{extent[0]}/{extent[1]}/{extent[2]}/{extent[3]}"
            
            # Extract coastline data
            coast_cmd = f"gmt pscoast -R{region_str} -JX10c -Dh -W0.1p -M > {coast_file.name}"
            result = subprocess.run(coast_cmd, shell=True, capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                coastline_segments = self._parse_gmt_file(coast_file.name)
                self._add_line_segments_to_axes(ax, coastline_segments, 'black', 0.8)
            
            # Extract river data
            river_cmd = f"gmt pscoast -R{region_str} -JX10c -Dh -I1/0.1p,blue -M > {river_file.name}"
            result = subprocess.run(river_cmd, shell=True, capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                river_segments = self._parse_gmt_file(river_file.name)
                self._add_line_segments_to_axes(ax, river_segments, 'blue', 0.5)
            
            # Clean up
            os.unlink(coast_file.name)
            os.unlink(river_file.name)
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è GMT feature extraction failed: {e}")
    
    def _parse_gmt_file(self, file_path):
        """Parse GMT multi-segment output file"""
        segments = []
        current_segment = []
        
        try:
            with open(file_path, 'r') as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    if line.startswith('>'):
                        if current_segment:
                            segments.append(np.array(current_segment))
                            current_segment = []
                        continue
                    try:
                        parts = line.split()
                        if len(parts) >= 2:
                            lon, lat = float(parts[0]), float(parts[1])
                            current_segment.append([lon, lat])
                    except (ValueError, IndexError):
                        continue
            
            if current_segment:
                segments.append(np.array(current_segment))
        except Exception:
            pass
        
        return segments
    
    def _add_line_segments_to_axes(self, ax, segments, color, linewidth):
        """Add line segments to matplotlib axes"""
        for segment in segments:
            if len(segment) > 1:
                ax.plot(segment[:, 0], segment[:, 1], color=color, linewidth=linewidth, alpha=0.8)
        
        # Map 1: Clay (Fine) Fraction Distribution
        ax = axes[0, 0]
        if 'fine_fraction' in geology:
            fine_content = geology['fine_fraction']['values']
            valid_mask = ~np.isnan(fine_content)
            
            if np.sum(valid_mask) > 0:
                scatter = ax.scatter(coordinates[valid_mask, 0], coordinates[valid_mask, 1], 
                                   c=fine_content[valid_mask], cmap='Blues', s=25, alpha=0.8,
                                   edgecolors='black', linewidth=0.2)
                cbar = plt.colorbar(scatter, ax=ax, shrink=0.8, pad=0.05)
                cbar.set_label('Fine Fraction (Clay Content)', fontsize=10)
                ax.set_title('Clay (Fine) Fraction Distribution', fontsize=12, fontweight='bold')
        
        # Map 2: Sand Fraction Distribution  
        ax = axes[0, 1]
        if 'sand_fraction' in geology:
            sand_content = geology['sand_fraction']['values']
            valid_mask = ~np.isnan(sand_content)
            
            if np.sum(valid_mask) > 0:
                scatter = ax.scatter(coordinates[valid_mask, 0], coordinates[valid_mask, 1], 
                                   c=sand_content[valid_mask], cmap='Greens', s=25, alpha=0.8,
                                   edgecolors='black', linewidth=0.2)
                cbar = plt.colorbar(scatter, ax=ax, shrink=0.8, pad=0.05)
                cbar.set_label('Sand Fraction', fontsize=10)
                ax.set_title('Sand Fraction Distribution', fontsize=12, fontweight='bold')
        
        # Map 3: Coarse Fraction Distribution
        ax = axes[1, 0]
        if 'coarse_fraction' in geology:
            coarse_content = geology['coarse_fraction']['values']
            valid_mask = ~np.isnan(coarse_content)
            
            if np.sum(valid_mask) > 0:
                scatter = ax.scatter(coordinates[valid_mask, 0], coordinates[valid_mask, 1], 
                                   c=coarse_content[valid_mask], cmap='Reds', s=25, alpha=0.8,
                                   edgecolors='black', linewidth=0.2)
                cbar = plt.colorbar(scatter, ax=ax, shrink=0.8, pad=0.05)
                cbar.set_label('Coarse Fraction', fontsize=10)
                ax.set_title('Coarse Fraction Distribution', fontsize=12, fontweight='bold')
        
        # Map 4: Subsidence Rates
        ax = axes[1, 1]
        valid_mask = ~np.isnan(subsidence_rates)
        if np.sum(valid_mask) > 0:
            scatter = ax.scatter(coordinates[valid_mask, 0], coordinates[valid_mask, 1], 
                               c=subsidence_rates[valid_mask], cmap='RdBu_r', s=25, alpha=0.8,
                               edgecolors='black', linewidth=0.2)
            cbar = plt.colorbar(scatter, ax=ax, shrink=0.8, pad=0.05)
            cbar.set_label('Subsidence Rate (mm/year)', fontsize=10)
            ax.set_title('InSAR Subsidence Rates', fontsize=12, fontweight='bold')
        
        plt.tight_layout()
        return fig

    def _create_seasonal_grain_size_analysis(self):
        """Create separate grain-size vs seasonality figures for each method and frequency band"""
        
        # Load all decomposition methods
        methods = ['emd', 'fft', 'vmd', 'wavelet']
        method_data = {}
        
        for method in methods:
            try:
                data_file = self.data_dir / f"ps02_{method}_decomposition.npz"
                if data_file.exists():
                    method_data[method] = np.load(data_file, allow_pickle=True)
            except Exception as e:
                print(f"‚ö†Ô∏è  Could not load {method} data: {e}")
        
        if not method_data:
            # Create a single fallback figure if no decomposition data available
            fig, ax = plt.subplots(1, 1, figsize=(10, 8))
            fig.suptitle('Grain-Size vs Seasonality Analysis - No Decomposition Data Available', fontsize=16)
            ax.text(0.5, 0.5, 'Decomposition data\nnot available\n\n(Run ps02 first)', 
                   ha='center', va='center', transform=ax.transAxes, fontsize=12,
                   bbox=dict(boxstyle="round", facecolor='lightgray', alpha=0.8))
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.set_xlabel('Grain Size Fraction')
            ax.set_ylabel('Seasonal Component Strength')
            plt.tight_layout()
            return [fig]  # Return list for consistency
        
        # Create separate figures for each method and frequency band
        geology = self.interpolated_geology['best']
        subsidence_rates = self.insar_data['subsidence_rates']
        
        # Extract grain-size data
        grain_sizes = {}
        for grain_type in ['fine_fraction', 'sand_fraction', 'coarse_fraction']:
            if grain_type in geology:
                grain_sizes[grain_type] = geology[grain_type]['values']
        
        if not grain_sizes:
            # Create a single fallback figure
            fig, ax = plt.subplots(1, 1, figsize=(10, 8))
            fig.suptitle('Grain-Size vs Seasonality Analysis - No Grain-Size Data Available', fontsize=16)
            ax.text(0.5, 0.5, 'Grain-size data\nnot available', 
                   ha='center', va='center', transform=ax.transAxes, fontsize=12,
                   bbox=dict(boxstyle="round", facecolor='lightgray', alpha=0.8))
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.set_xlabel('Grain Size Fraction')
            ax.set_ylabel('Seasonal Component Strength')
            plt.tight_layout()
            return [fig]
        
        # Define frequency bands (in days) matching ps02 analysis
        frequency_bands = {
            'quarterly': (60, 120),      # Quarterly patterns
            'semi_annual': (120, 280),   # Semi-annual patterns  
            'annual': (280, 400),        # Annual patterns
        }
        
        figures = []
        
        # Create individual figures for each method and frequency band
        for method in method_data.keys():
            method_upper = method.upper()
            
            for band_name, band_range in frequency_bands.items():
                # Create figure for this method-band combination
                fig, ax = plt.subplots(1, 1, figsize=(10, 8))
                
                band_display = band_name.replace('_', '-').title()
                fig.suptitle(f'{method_upper} {band_display} Components vs Grain-Size Fractions', 
                           fontsize=14, fontweight='bold')
                
                # Plot seasonal component vs grain sizes for this method and band
                self._plot_method_band_vs_grain_sizes(ax, method_data[method], band_name, band_range, grain_sizes, method)
                
                plt.tight_layout()
                figures.append((fig, f'ps08_fig05_{method}_{band_name}_grain_size.png'))
        
        # Also create a reference figure with direct subsidence vs grain sizes
        fig, ax = plt.subplots(1, 1, figsize=(10, 8))
        fig.suptitle('Direct Subsidence Rates vs Grain-Size Fractions (Reference)', 
                   fontsize=14, fontweight='bold')
        self._plot_subsidence_vs_grain_sizes(ax, subsidence_rates, grain_sizes)
        plt.tight_layout()
        figures.append((fig, 'ps08_fig05_subsidence_reference_grain_size.png'))
        
        # Save all figures
        for fig, filename in figures:
            fig_path = self.figures_dir / filename
            fig.savefig(fig_path, dpi=300, bbox_inches='tight')
            plt.close(fig)
            print(f"   ‚úÖ Created: {filename}")
        
        print(f"üìä Created {len(figures)} seasonal grain-size analysis figures")
        return figures
    
    def _plot_method_band_vs_grain_sizes(self, ax, method_data, band_name, band_range, grain_sizes, method_name):
        """Plot seasonal component amplitudes vs grain sizes using ps02 recategorization results"""
        
        print(f"üìä Processing {method_name.upper()} {band_name} components...")
        
        try:
            import json
            recat_file = f"data/processed/ps02_{method_name}_recategorization.json"
            with open(recat_file, 'r') as f:
                recategorization = json.load(f)
            
            imfs = method_data['imfs']
            n_stations = len(imfs)
            seasonal_strengths = []
            
            for station_idx in range(n_stations):
                station_imfs = imfs[station_idx]
                if station_imfs is None or len(station_imfs) == 0:
                    seasonal_strengths.append(0)
                    continue
                
                # Get recategorization for this station
                station_recat = recategorization.get(str(station_idx), {})
                
                # Sum strength of IMFs that belong to the target frequency band
                band_strength = 0
                for imf_idx, imf in enumerate(station_imfs):
                    if imf is not None and len(imf) > 10 and not np.all(np.isnan(imf)):
                        imf_key = f"imf_{imf_idx}"
                        if imf_key in station_recat:
                            imf_category = station_recat[imf_key].get('final_category', '')
                            
                            # Map band_name to ps02 categories
                            if ((band_name == 'quarterly' and imf_category == 'quarterly') or
                                (band_name == 'semi_annual' and imf_category == 'semi_annual') or  
                                (band_name == 'annual' and imf_category == 'annual')):
                                band_strength += np.std(imf)  # Use standard deviation as strength measure
                
                seasonal_strengths.append(band_strength)
            
            # Debug: Show statistics for this method and band
            seasonal_strengths_array = np.array(seasonal_strengths)
            non_zero_count = np.sum(seasonal_strengths_array > 0)
            print(f"   {method_name.upper()} {band_name}: {non_zero_count}/{len(seasonal_strengths)} stations have components (max: {np.max(seasonal_strengths_array):.2f})")
            
        except (FileNotFoundError, json.JSONDecodeError, KeyError) as e:
            print(f"‚ö†Ô∏è  Could not load ps02 recategorization for {method_name}: {e}")
            print(f"   Falling back to period calculation...")
            
            # Fallback to original period calculation approach
            imfs = method_data['imfs']
            n_stations = len(imfs)
            seasonal_strengths = []
            
            for station_idx in range(n_stations):
                station_imfs = imfs[station_idx]
                if station_imfs is None or len(station_imfs) == 0:
                    seasonal_strengths.append(0)
                    continue
                    
                # Calculate power spectral density for each IMF
                band_strength = 0
                for imf in station_imfs:
                    if imf is not None and len(imf) > 10 and not np.all(np.isnan(imf)):
                        # Calculate dominant period using zero-crossings
                        zero_crossings = np.where(np.diff(np.sign(imf)))[0]
                        if len(zero_crossings) > 2:
                            avg_period = 2 * len(imf) / len(zero_crossings)  # Approximate period
                            
                            # Check if this IMF falls within the target frequency band
                            if band_range[0] <= avg_period <= band_range[1]:
                                band_strength += np.std(imf)  # Use standard deviation as strength measure
                
                seasonal_strengths.append(band_strength)
        
        seasonal_strengths = np.array(seasonal_strengths)
        
        # Special case: For EMD annual, use log(fine/coarse) ratio
        if method_name.lower() == 'emd' and band_name == 'annual':
            if 'fine_fraction' in grain_sizes and 'coarse_fraction' in grain_sizes:
                fine_values = grain_sizes['fine_fraction']
                coarse_values = grain_sizes['coarse_fraction']
                
                # Calculate fine/coarse ratio (avoid division by zero)
                fine_coarse_ratio = np.zeros_like(fine_values)
                valid_coarse = coarse_values > 1e-6  # Avoid division by very small numbers
                fine_coarse_ratio[valid_coarse] = fine_values[valid_coarse] / coarse_values[valid_coarse]
                
                # Calculate log(fine/coarse) ratio (avoid log of zero or negative values)
                log_fine_coarse_ratio = np.full_like(fine_coarse_ratio, np.nan)
                valid_ratio = (fine_coarse_ratio > 1e-6) & (fine_coarse_ratio < 1000)  # Remove extreme ratios
                log_fine_coarse_ratio[valid_ratio] = np.log10(fine_coarse_ratio[valid_ratio])
                
                # Filter out invalid data
                valid_mask = (~(np.isnan(fine_values) | np.isnan(coarse_values) | np.isnan(seasonal_strengths) | 
                               np.isnan(log_fine_coarse_ratio)) & valid_coarse & valid_ratio)
                
                if np.sum(valid_mask) > 10:  # Need at least 10 points
                    x_vals = log_fine_coarse_ratio[valid_mask]
                    y_vals = seasonal_strengths[valid_mask]
                    
                    # Scatter plot
                    ax.scatter(x_vals, y_vals, c='purple', alpha=0.8, s=20, 
                             facecolors='none', edgecolors='purple', linewidths=0.8, 
                             label='log‚ÇÅ‚ÇÄ(Fine/Coarse)')
                    
                    # Calculate and show correlation
                    if len(x_vals) > 3:
                        correlation = np.corrcoef(x_vals, y_vals)[0, 1]
                        if not np.isnan(correlation):
                            print(f"{band_name.title()} vs log(Fine/Coarse): r={correlation:.3f}")
                            
                            # Add trend line if significant correlation
                            from scipy import stats as scipy_stats
                            r, p = scipy_stats.pearsonr(x_vals, y_vals)
                            if p < 0.05:  # Significant correlation
                                z = np.polyfit(x_vals, y_vals, 1)
                                p_line = np.poly1d(z)
                                x_trend = np.linspace(x_vals.min(), x_vals.max(), 100)
                                ax.plot(x_trend, p_line(x_trend), color='purple', linestyle='-', alpha=0.7)
                
                ax.set_xlabel('log‚ÇÅ‚ÇÄ(Fine/Coarse Ratio)', fontsize=12)
        else:
            # Plot vs each grain size type (original behavior for other cases)
            colors = ['red', 'orange', 'brown']
            grain_labels = ['Fine (Clay)', 'Sand', 'Coarse']
            
            for idx, (grain_type, color, label) in enumerate(zip(['fine_fraction', 'sand_fraction', 'coarse_fraction'], 
                                                               colors, grain_labels)):
                if grain_type in grain_sizes:
                    grain_values = grain_sizes[grain_type]
                    
                    # Filter out invalid data
                    valid_mask = ~(np.isnan(grain_values) | np.isnan(seasonal_strengths))
                    if np.sum(valid_mask) > 10:  # Need at least 10 points
                        x_vals = grain_values[valid_mask]
                        y_vals = seasonal_strengths[valid_mask]
                        
                        # Scatter plot with unfilled circles for better visibility
                        ax.scatter(x_vals, y_vals, c=color, alpha=0.8, s=20, 
                                 facecolors='none', edgecolors=color, linewidths=0.8, label=label)
                        
                        # Calculate and show correlation
                        if len(x_vals) > 3:
                            correlation = np.corrcoef(x_vals, y_vals)[0, 1]
                            if not np.isnan(correlation):
                                print(f"{band_name.title()} vs {label}: r={correlation:.3f}")
            
            ax.set_xlabel('Grain Size Fraction', fontsize=12)
        ax.set_ylabel(f'{band_name.replace("_", "-").title()} Component Strength', fontsize=12)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # Add zero/non-zero statistics
        zero_count = np.sum(seasonal_strengths == 0)
        non_zero_count = np.sum(seasonal_strengths > 0)
        total_count = len(seasonal_strengths)
        zero_pct = (zero_count / total_count) * 100
        non_zero_pct = (non_zero_count / total_count) * 100
        
        stats_text = f"Signal Statistics:\n"
        stats_text += f"No Signal: {zero_count}/{total_count} ({zero_pct:.1f}%)\n"
        stats_text += f"With Signal: {non_zero_count}/{total_count} ({non_zero_pct:.1f}%)"
        
        ax.text(0.02, 0.02, stats_text, transform=ax.transAxes, fontsize=12,
                verticalalignment='bottom', horizontalalignment='left', 
                bbox=dict(boxstyle="round,pad=0.5", facecolor='yellow', 
                         edgecolor='black', alpha=0.95, linewidth=1))
    
    def _plot_seasonal_vs_grain_sizes(self, ax, method_data, band_name, band_range, grain_sizes):
        """Plot seasonal component amplitudes vs grain sizes for all methods"""
        
        colors = {'emd': 'red', 'fft': 'blue', 'vmd': 'green', 'wavelet': 'orange'}
        markers = {'fine_fraction': 'o', 'sand_fraction': 's', 'coarse_fraction': '^'}
        
        for method, color in colors.items():
            if method not in method_data:
                continue
                
            try:
                # Extract seasonal component for this method and frequency band
                seasonal_amplitudes = self._extract_seasonal_component(method_data[method], band_range)
                
                if seasonal_amplitudes is None:
                    continue
                
                # Plot vs each grain size
                for grain_type, marker in markers.items():
                    if grain_type not in grain_sizes:
                        continue
                    
                    grain_values = grain_sizes[grain_type]
                    
                    # Find valid data (both seasonal and grain-size available)
                    valid_mask = ~(np.isnan(seasonal_amplitudes) | np.isnan(grain_values))
                    
                    if np.sum(valid_mask) < 10:  # Need at least 10 points
                        continue
                    
                    # Plot scatter
                    label = f'{method.upper()}-{grain_type.split("_")[0].title()}'
                    ax.scatter(grain_values[valid_mask], seasonal_amplitudes[valid_mask], 
                             c=color, marker=marker, alpha=0.6, s=30, label=label)
                    
                    # Add correlation if significant
                    if np.sum(valid_mask) > 20:
                        r, p = stats.pearsonr(grain_values[valid_mask], seasonal_amplitudes[valid_mask])
                        if p < 0.05:  # Significant correlation
                            # Add trend line
                            z = np.polyfit(grain_values[valid_mask], seasonal_amplitudes[valid_mask], 1)
                            p_line = np.poly1d(z)
                            x_trend = np.linspace(grain_values[valid_mask].min(), grain_values[valid_mask].max(), 100)
                            ax.plot(x_trend, p_line(x_trend), color=color, linestyle='--', alpha=0.5)
            
            except Exception as e:
                print(f"‚ö†Ô∏è  Error processing {method} for {band_name}: {e}")
                continue
        
        ax.set_xlabel('Grain-Size Fraction')
        ax.set_ylabel('Seasonal Amplitude (mm)')
        ax.grid(True, alpha=0.3)
        
        # Add legend if there are any plots
        handles, labels = ax.get_legend_handles_labels()
        if handles:
            ax.legend(fontsize=8, ncol=2)
    
    def _plot_subsidence_vs_grain_sizes(self, ax, subsidence_rates, grain_sizes):
        """Plot subsidence rates vs grain sizes (reference plot)"""
        
        markers = {'fine_fraction': 'o', 'sand_fraction': 's', 'coarse_fraction': '^'}
        colors = {'fine_fraction': 'brown', 'sand_fraction': 'gold', 'coarse_fraction': 'gray'}
        
        for grain_type, marker in markers.items():
            if grain_type not in grain_sizes:
                continue
                
            grain_values = grain_sizes[grain_type]
            valid_mask = ~(np.isnan(subsidence_rates) | np.isnan(grain_values))
            
            if np.sum(valid_mask) < 10:
                continue
            
            color = colors[grain_type]
            label = grain_type.split('_')[0].title()
            
            ax.scatter(grain_values[valid_mask], subsidence_rates[valid_mask], 
                     marker=marker, alpha=0.8, s=30, label=label,
                     facecolors='none', edgecolors=color, linewidths=0.8)
            
            # Add correlation info
            if np.sum(valid_mask) > 20:
                r, p = stats.pearsonr(grain_values[valid_mask], subsidence_rates[valid_mask])
                print(f'Subsidence vs {label}: r={r:.3f}, p={p:.3f}')
                
                if p < 0.05:  # Significant correlation
                    # Add trend line
                    z = np.polyfit(grain_values[valid_mask], subsidence_rates[valid_mask], 1)
                    p_line = np.poly1d(z)
                    x_trend = np.linspace(grain_values[valid_mask].min(), grain_values[valid_mask].max(), 100)
                    ax.plot(x_trend, p_line(x_trend), color=color, linestyle='-', alpha=0.7)
        
        ax.set_xlabel('Grain-Size Fraction')
        ax.set_ylabel('Subsidence Rate (mm/year)')
        ax.grid(True, alpha=0.3)
        ax.legend()
        
        # Add subsidence rate statistics
        subsiding_count = np.sum(subsidence_rates < -1.0)  # Significant subsidence (< -1 mm/year)
        stable_count = np.sum(np.abs(subsidence_rates) <= 1.0)  # Stable (-1 to +1 mm/year)
        uplifting_count = np.sum(subsidence_rates > 1.0)  # Significant uplift (> +1 mm/year)
        total_count = len(subsidence_rates)
        
        subsiding_pct = (subsiding_count / total_count) * 100
        stable_pct = (stable_count / total_count) * 100
        uplifting_pct = (uplifting_count / total_count) * 100
        
        stats_text = f"Deformation Statistics:\n"
        stats_text += f"Subsiding (< -1 mm/yr): {subsiding_count} ({subsiding_pct:.1f}%)\n"
        stats_text += f"Stable (¬±1 mm/yr): {stable_count} ({stable_pct:.1f}%)\n"
        stats_text += f"Uplifting (> +1 mm/yr): {uplifting_count} ({uplifting_pct:.1f}%)"
        
        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=12,
                verticalalignment='top', horizontalalignment='left', 
                bbox=dict(boxstyle="round,pad=0.5", facecolor='lightgreen', 
                         edgecolor='black', alpha=0.95, linewidth=1))
    
    def _extract_seasonal_component(self, method_data, band_range):
        """Extract seasonal component amplitude for given frequency band"""
        
        if band_range is None:
            return None
            
        try:
            # This is a simplified extraction - in practice you'd need to:
            # 1. Load IMFs or frequency components from the method data
            # 2. Filter by the frequency band
            # 3. Calculate amplitude/energy in that band
            
            # For now, return a placeholder that simulates seasonal amplitudes
            # In real implementation, this would extract actual seasonal signals
            
            # Simulate seasonal amplitudes based on frequency band
            n_stations = len(self.insar_coordinates)
            
            # Create synthetic seasonal amplitudes for demonstration
            # Lower frequencies (annual) typically have higher amplitudes
            if band_range[0] > 200:  # Annual
                amplitudes = np.random.normal(5.0, 2.0, n_stations)
            elif band_range[0] > 100:  # Semi-annual  
                amplitudes = np.random.normal(3.0, 1.5, n_stations)
            else:  # Quarterly
                amplitudes = np.random.normal(2.0, 1.0, n_stations)
            
            # Add some NaN values to simulate missing data
            mask = np.random.random(n_stations) > 0.6  # 60% coverage
            amplitudes[~mask] = np.nan
            
            return amplitudes
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Error extracting seasonal component: {e}")
            return None
            
    def save_results(self):
        """Save all analysis results to files"""
        print("üíæ Saving geological integration results...")
        
        try:
            # Save interpolation results
            if self.interpolated_geology:
                interp_file = self.results_dir / "interpolation_results" / "interpolated_geology.json"
                with open(interp_file, 'w') as f:
                    # Convert numpy arrays to lists for JSON serialization
                    serializable_data = {}
                    for method, method_data in self.interpolated_geology.items():
                        serializable_data[method] = {}
                        for prop, prop_data in method_data.items():
                            serializable_data[method][prop] = {}
                            for key, value in prop_data.items():
                                if isinstance(value, np.ndarray):
                                    serializable_data[method][prop][key] = value.tolist()
                                else:
                                    serializable_data[method][prop][key] = value
                    
                    json.dump(serializable_data, f, indent=2)
                print(f"‚úÖ Saved interpolation results: {interp_file}")
            
            # Save correlation results
            if self.correlation_results:
                corr_file = self.results_dir / "correlation_analysis" / "geology_deformation_correlations.json"
                with open(corr_file, 'w') as f:
                    json.dump(self.correlation_results, f, indent=2, default=str)
                print(f"‚úÖ Saved correlation results: {corr_file}")
            
            # Save process interpretation
            if self.process_interpretation:
                process_file = self.results_dir / "process_interpretation" / "geological_process_analysis.json"
                with open(process_file, 'w') as f:
                    # Convert numpy arrays to lists
                    serializable_data = {}
                    for key, value in self.process_interpretation.items():
                        if isinstance(value, dict):
                            serializable_data[key] = {}
                            for subkey, subvalue in value.items():
                                if isinstance(subvalue, np.ndarray):
                                    serializable_data[key][subkey] = subvalue.tolist()
                                else:
                                    serializable_data[key][subkey] = subvalue
                        else:
                            serializable_data[key] = value
                    
                    json.dump(serializable_data, f, indent=2, default=str)
                print(f"‚úÖ Saved process interpretation: {process_file}")
            
            # Save borehole data summary
            borehole_summary = {
                'n_boreholes': self.borehole_data['n_boreholes'],
                'coordinate_bounds': {
                    'longitude': [float(self.borehole_data['coordinates'][:, 0].min()), 
                                 float(self.borehole_data['coordinates'][:, 0].max())],
                    'latitude': [float(self.borehole_data['coordinates'][:, 1].min()), 
                                float(self.borehole_data['coordinates'][:, 1].max())]
                },
                'geological_properties': list(self.borehole_data['geological_properties'].keys())
            }
            
            borehole_file = self.results_dir / "validation" / "borehole_data_summary.json"
            with open(borehole_file, 'w') as f:
                json.dump(borehole_summary, f, indent=2)
            print(f"‚úÖ Saved borehole summary: {borehole_file}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error saving results: {e}")
            return False

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description='Geological Integration Analysis for Taiwan Subsidence')
    parser.add_argument('--methods', type=str, default='emd',
                       help='Comma-separated list of methods: emd,fft,vmd,wavelet or "all"')
    parser.add_argument('--max-distance', type=float, default=15.0,
                       help='Maximum distance for interpolation (km, default: 15.0)')
    parser.add_argument('--interpolation-method', type=str, default='idw',
                       choices=['idw', 'kriging', 'both'],
                       help='Interpolation method (default: idw)')
    parser.add_argument('--create-visualizations', action='store_true',
                       help='Create visualization figures')
    parser.add_argument('--save-results', action='store_true',
                       help='Save analysis results to files')
    return parser.parse_args()

def main():
    """Main geological integration analysis workflow"""
    args = parse_arguments()
    
    # Parse methods
    if args.methods.lower() == 'all':
        methods = ['emd', 'fft', 'vmd', 'wavelet']
    else:
        methods = [m.strip().lower() for m in args.methods.split(',')]
    
    print("=" * 80)
    print("üóª ps08_geological_integration.py - Geological Integration Analysis")
    print(f"üìã METHODS: {', '.join(methods).upper()}")
    print(f"üìè MAX DISTANCE: {args.max_distance} km")
    print(f"üîß INTERPOLATION: {args.interpolation_method}")
    print("=" * 80)
    
    # Initialize analysis
    geo_analysis = GeologicalIntegrationAnalysis(
        methods=methods,
        max_distance_km=args.max_distance,
        interpolation_method=args.interpolation_method
    )
    
    # Load data
    print("\nüîÑ LOADING DATA")
    print("-" * 50)
    if not geo_analysis.load_insar_data():
        print("‚ùå Failed to load InSAR data")
        return False
    
    if not geo_analysis.load_borehole_data():
        print("‚ùå Failed to load borehole data")
        return False
    
    # Spatial interpolation
    print("\nüîÑ SPATIAL INTERPOLATION")
    print("-" * 50)
    if not geo_analysis.interpolate_geology_to_insar():
        print("‚ùå Failed to perform spatial interpolation")
        return False
    
    # Correlation analysis
    print("\nüîÑ CORRELATION ANALYSIS")
    print("-" * 50)
    if not geo_analysis.perform_correlation_analysis():
        print("‚ùå Failed to perform correlation analysis")
        return False
    
    # Process interpretation
    print("\nüîÑ GEOLOGICAL PROCESS INTERPRETATION")
    print("-" * 50)
    if not geo_analysis.interpret_geological_processes():
        print("‚ùå Failed to interpret geological processes")
        return False
    
    # Create visualizations
    if args.create_visualizations:
        print("\nüîÑ CREATING VISUALIZATIONS")
        print("-" * 50)
        if not geo_analysis.create_geological_correlation_visualizations():
            print("‚ö†Ô∏è  Failed to create some visualizations")
        
        # Create seasonal grain-size analysis figures
        print("\nüîÑ CREATING SEASONAL GRAIN-SIZE ANALYSIS")
        print("-" * 50)
        try:
            geo_analysis._create_seasonal_grain_size_analysis()
            print("‚úÖ Seasonal grain-size analysis figures created successfully")
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to create seasonal grain-size analysis: {e}")
    
    # Save results
    if args.save_results:
        print("\nüíæ SAVING RESULTS")
        print("-" * 50)
        geo_analysis.save_results()
    
    print("\n" + "=" * 80)
    print("‚úÖ ps08_geological_integration.py ANALYSIS COMPLETED SUCCESSFULLY")
    
    # Print summary
    if hasattr(geo_analysis, 'correlation_results') and 'subsidence_rate_correlations' in geo_analysis.correlation_results:
        print("\nüìä CORRELATION SUMMARY:")
        correlations = geo_analysis.correlation_results['subsidence_rate_correlations']
        for prop, corr_data in correlations.items():
            r = corr_data['pearson_r']
            p = corr_data['pearson_p']
            sig = "significant" if p < 0.05 else "not significant"
            print(f"   {prop.replace('_', ' ').title()}: r = {r:.3f}, p = {p:.3f} ({sig})")
    
    if hasattr(geo_analysis, 'susceptibility_analysis') and 'class_distributions' in geo_analysis.susceptibility_analysis:
        print("\nüö® SUSCEPTIBILITY SUMMARY:")
        class_dist = geo_analysis.susceptibility_analysis['class_distributions']
        total_stations = sum(class_dist.values())
        for sclass, count in class_dist.items():
            percentage = count / total_stations * 100
            print(f"   {sclass.replace('_', ' ').title()}: {count} stations ({percentage:.1f}%)")
    
    print("\nüìä Generated outputs:")
    print("   1. Geological-deformation correlation analysis")
    print("   2. Geological susceptibility maps")
    print("   3. Geological process interpretation")
    print("   4. Interpolation quality assessment")
    if args.create_visualizations:
        print("   5. Seasonal grain-size analysis (by method & frequency band):")
        methods = ['emd', 'fft', 'vmd', 'wavelet']
        bands = ['quarterly', 'semi_annual', 'annual']
        for method in methods:
            for band in bands:
                print(f"      - ps08_fig05_{method}_{band}_grain_size.png")
        print("      - ps08_fig05_subsidence_reference_grain_size.png")
    print("=" * 80)
    
    return True

if __name__ == "__main__":
    success = main()
    if not success:
        exit(1)